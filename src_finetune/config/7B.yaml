# data
data:
    instruct_data: "/work/pi_mchiovaro_uri_edu/richard_buckley/musicgen/jsonl_data/part_00000.jsonl,/work/pi_mchiovaro_uri_edu/richard_buckley/musicgen/jsonl_data/part_00002.jsonl,/work/pi_mchiovaro_uri_edu/richard_buckley/musicgen/jsonl_data/part_00003.jsonl"
    data: ""
    eval_instruct_data: "" # Optionally fill

# model
model_id_or_path: "/work/pi_mchiovaro_uri_edu/richard_buckley/musicgen/mistral_models"
lora:
    rank: 64

# optim
seq_len: 32768
batch_size: 1
max_steps: 300
optim:
    lr: 6.e-5
    weight_decay: 0.1
    pct_start: 0.05

# other
seed: 0
log_freq: 1
eval_freq: 100
no_eval: False
ckpt_freq: 100

save_adapters: True # save only trained LoRA adapters. Set to `False` to merge LoRA adapter into the base model and save full fine-tuned model

run_dir: "/work/pi_mchiovaro_uri_edu/richard_buckley/musicgen/mistral_finetune" # Fill

wandb:
    project: "text2midi-llm" # your wandb project name
    run_name: "7B-finetune" # your wandb run name
    key: "e2058c09a28e4fb17341996da05b5f38236f031a" # your wandb api key
    offline: False
