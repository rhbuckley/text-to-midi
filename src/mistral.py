import glob
import json
import os
import unsloth
import pretty_midi
import soundfile as sf
from datasets import load_dataset

# Constants for token representation
TIME_RESOLUTION = 100  # Steps per second
MAX_TIME_SHIFT = 100  # Maximum time shift in steps (1 second)
VELOCITY_BINS = 32  # Number of bins to quantize velocity


PIECE_START = "<>"
PIECE_END = "</>"
INSTRUMENT = "I"
TIME_SHIFT = "T"
VELOCITY = "V"
NOTE_ON = "N"
NOTE_OFF = "O"


def quantize_velocity(velocity, bins=VELOCITY_BINS):
    """Quantize velocity into discrete bins."""
    if velocity < 0:
        velocity = 0
    if velocity > 127:
        velocity = 127
    return min(int(velocity / (128 / bins)), bins - 1)


def dequantize_velocity(bin_index, bins=VELOCITY_BINS):
    """Dequantize velocity from bin index."""
    # Return the middle value of the bin
    return int((bin_index + 0.5) * (128 / bins))


def encode_midi_to_tokens(midi_path, time_resolution=TIME_RESOLUTION):
    """
    Encodes a MIDI file into a sequence of tokens suitable for language model processing.

    The token format uses special markers:
    - `PIECE_START`: Marks the beginning of the MIDI sequence.
    - `INSTRUMENT=<program>`: Selects the active instrument (MIDI program number).
    - `TIME_SHIFT=<steps>`: Advances time by the specified number of steps. Time is quantized
                             based on `time_resolution`. Maximum shift is `MAX_TIME_SHIFT`.
    - `NOTE_ON=<pitch>`: Starts a note with the given MIDI pitch.
    - `NOTE_OFF=<pitch>`: Stops the note with the given MIDI pitch.
    - `VELOCITY=<bin>`: Sets the velocity for the *next* NOTE_ON event. Velocity is
                        quantized into `VELOCITY_BINS` bins.
    - `PIECE_END`: Marks the end of the MIDI sequence.

    Args:
        midi_path (str): Path to the MIDI file.
        time_resolution (int): Steps per second for time quantization.

    Returns:
        list[str]: A list of string tokens representing the MIDI events.
    """
    try:
        midi = pretty_midi.PrettyMIDI(midi_path)
    except Exception as e:
        print(f"Error loading MIDI file {midi_path}: {e}")
        return []  # Return empty list on error

    events = []
    for instrument in midi.instruments:
        # Sort notes by start time, then pitch (for determinism)
        sorted_notes = sorted(instrument.notes, key=lambda x: (x.start, x.pitch))
        for note in sorted_notes:
            start_step = round(note.start * time_resolution)
            end_step = round(note.end * time_resolution)
            velocity_bin = quantize_velocity(note.velocity)

            # Add events: (time_step, type, value, instrument_program)
            events.append(
                (start_step, INSTRUMENT, instrument.program, instrument.program)
            )
            events.append((start_step, VELOCITY, velocity_bin, instrument.program))
            events.append((start_step, NOTE_ON, note.pitch, instrument.program))
            events.append((end_step, NOTE_OFF, note.pitch, instrument.program))

    if not events:
        return [PIECE_START, PIECE_END]

    # Sort all events by time, then by type priority (INSTRUMENT, VELOCITY, NOTE_ON, NOTE_OFF)
    type_priority = {INSTRUMENT: 0, VELOCITY: 1, NOTE_ON: 2, NOTE_OFF: 3}
    events.sort(key=lambda x: (x[0], type_priority.get(x[1], 99)))

    tokens = [PIECE_START]
    current_time_step = 0

    for time_step, event_type, value, instr_prog in events:
        # --- Add Time Shift ---
        time_diff = time_step - current_time_step
        if time_diff > 0:
            # Decompose large time shifts into smaller chunks
            while time_diff > 0:
                shift = min(time_diff, MAX_TIME_SHIFT)
                tokens.append(f"{TIME_SHIFT}={shift}")
                time_diff -= shift
                current_time_step += shift  # Update current time *after* adding token

        # --- Add Instrument Change (if needed) ---
        # Only add INSTRUMENT token if it changes *for this specific event's instrument*
        # This logic might need refinement depending on how multi-instrument handling is desired downstream.
        # A simpler approach might be to always emit INSTRUMENT before the first event of that instrument.
        # For now, let's assume the model learns the context.
        # if instr_prog != active_instrument: # Re-enable if explicit instrument switching is needed per event
        #    tokens.append(f"INSTRUMENT={instr_prog}")
        #    active_instrument = instr_prog

        # --- Add Event Token ---
        tokens.append(f"{event_type}={value}")

        # Update current_time_step *after* processing events at this step
        # current_time_step = time_step # This was updated during time shift handling

    tokens.append(PIECE_END)
    return tokens


def decode_tokens_to_midi(
    tokens, time_resolution=TIME_RESOLUTION
) -> pretty_midi.PrettyMIDI:
    """
    Decodes a sequence of tokens (generated by `encode_midi_to_tokens`) back into a MIDI object.
    Handles overlapping notes correctly.

    Args:
        tokens (list[str]): The list of string tokens.
        time_resolution (int): Steps per second used during encoding.

    Returns:
        pretty_midi.PrettyMIDI: The decoded MIDI object.
    """
    midi = pretty_midi.PrettyMIDI()
    instruments = {}  # Dictionary to hold instruments {program: Instrument}
    current_time = 0.0
    current_instrument_program = 0  # Default instrument program
    # Store active notes as a list associated with each (instrument, pitch) pair
    # active_notes = {(instrument_program, pitch): [(start_time1, velocity1), (start_time2, velocity2), ...]}
    active_notes = {}
    pending_velocity = 64  # Default velocity if not specified

    for token in tokens:
        if token == PIECE_START or token == PIECE_END:
            continue

        try:
            event_type, value = token.split("=", 1)
        except ValueError:
            print(f"Warning: Skipping malformed token: {token}")
            continue

        if event_type == INSTRUMENT:
            try:
                current_instrument_program = int(value)
                if current_instrument_program not in instruments:
                    instruments[current_instrument_program] = pretty_midi.Instrument(
                        program=current_instrument_program
                    )
            except ValueError:
                print(f"Warning: Invalid instrument program value: {value}. Skipping.")
        elif event_type == TIME_SHIFT:
            try:
                steps = int(value)
                if steps < 0:
                    print(
                        f"Warning: Negative time shift {steps} encountered. Skipping."
                    )
                    continue
                current_time += steps / time_resolution
            except ValueError:
                print(f"Warning: Invalid time shift value: {value}. Skipping.")
        elif event_type == VELOCITY:
            try:
                velocity_bin = int(value)
                if 0 <= velocity_bin < VELOCITY_BINS:
                    pending_velocity = dequantize_velocity(velocity_bin)
                else:
                    print(
                        f"Warning: Velocity bin {velocity_bin} out of range [0, {VELOCITY_BINS-1}]. Using default."
                    )
                    pending_velocity = 64
            except ValueError:
                print(f"Warning: Invalid velocity value: {value}. Skipping.")
                pending_velocity = 64  # Use default on error

        elif event_type == NOTE_ON:
            try:
                pitch = int(value)
                if not (0 <= pitch <= 127):
                    print(f"Warning: Invalid pitch value {pitch}. Skipping NOTE_ON.")
                    continue

                if current_instrument_program not in instruments:
                    print(
                        f"Warning: NOTE_ON for pitch {pitch} encountered before INSTRUMENT definition for program {current_instrument_program}. Creating default instrument."
                    )
                    instruments[current_instrument_program] = pretty_midi.Instrument(
                        program=current_instrument_program
                    )

                note_key = (current_instrument_program, pitch)
                # Initialize list if this is the first note for this key
                if note_key not in active_notes:
                    active_notes[note_key] = []

                # Add the new note start information to the list
                active_notes[note_key].append((current_time, pending_velocity))

                # Sort the list by start time to ensure NOTE_OFF pairs with the earliest NOTE_ON
                active_notes[note_key].sort(key=lambda x: x[0])

                # Reset pending velocity after use? Or keep it for subsequent notes? Let's keep it.
                # pending_velocity = 64 # Uncomment to reset velocity after each NOTE_ON
            except ValueError:
                print(f"Warning: Invalid pitch value: {value}. Skipping NOTE_ON.")
        elif event_type == NOTE_OFF:
            try:
                pitch = int(value)
                if not (0 <= pitch <= 127):
                    print(f"Warning: Invalid pitch value {pitch}. Skipping NOTE_OFF.")
                    continue

                note_key = (current_instrument_program, pitch)
                # Check if there are active notes for this key
                if note_key in active_notes and active_notes[note_key]:
                    # Retrieve the earliest started note (FIFO)
                    start_time, velocity = active_notes[note_key].pop(
                        0
                    )  # Remove the first element

                    # Ensure end time is not before start time
                    end_time = max(start_time, current_time)
                    if end_time > start_time:  # Avoid zero-duration notes
                        note = pretty_midi.Note(
                            velocity=velocity,
                            pitch=pitch,
                            start=start_time,
                            end=end_time,
                        )
                        if current_instrument_program in instruments:
                            instruments[current_instrument_program].notes.append(note)
                        else:
                            # Should not happen if NOTE_ON created the instrument, but handle defensively
                            print(
                                f"Error: Instrument {current_instrument_program} not found for NOTE_OFF. Discarding note."
                            )
                    # else: Note has zero or negative duration, discard.

                    # Clean up dictionary entry if list becomes empty
                    if not active_notes[note_key]:
                        del active_notes[note_key]
                else:
                    # Note OFF event without a corresponding active Note ON. Ignore or log.
                    # This can happen if the MIDI data is unusual or encoding/decoding has issues.
                    print(
                        f"Warning: NOTE_OFF for pitch {pitch} without active NOTE_ON for instrument {current_instrument_program} at time {current_time}. Skipping."
                    )
            except ValueError:
                print(f"Warning: Invalid pitch value: {value}. Skipping NOTE_OFF.")
        else:
            print(f"Warning: Unknown event type: {event_type}. Skipping token: {token}")

    # After processing all tokens, handle any remaining NOTE_ON events
    # (notes that started but never received a NOTE_OFF).
    # End them at the current_time.
    for (instr_prog, pitch), remaining_notes in active_notes.items():
        # print(f"Warning: Found {len(remaining_notes)} dangling NOTE_ON event(s) for instrument {instr_prog}, pitch {pitch}. Ending at time {current_time}.")
        for start_time, velocity in remaining_notes:
            # Ensure end time is not before start time
            end_time = max(start_time, current_time)
            if end_time > start_time:  # Avoid zero-duration notes
                note = pretty_midi.Note(
                    velocity=velocity, pitch=pitch, start=start_time, end=end_time
                )
                if instr_prog in instruments:
                    instruments[instr_prog].notes.append(note)
                else:
                    # This case should ideally not be reached if NOTE_ON created the instrument
                    print(
                        f"Error: Instrument {instr_prog} not found for dangling NOTE_ON cleanup. Discarding note."
                    )
            # else: Note has zero or negative duration, discard.

    # Add all created instruments to the MIDI object
    for instrument in instruments.values():
        if instrument.notes:  # Only add instruments that have notes
            # Sort notes within each instrument before adding (good practice)
            instrument.notes.sort(key=lambda note: note.start)
            midi.instruments.append(instrument)

    # If no instruments were added, return empty MIDI
    # if not midi.instruments:
    #    midi.instruments.append(pretty_midi.Instrument(program=0))

    return midi


def pretty_midi_to_wav(midi: pretty_midi.PrettyMIDI, output_path: str, fs: int = 44100):
    """
    Convert a pretty_midi.PrettyMIDI object to a WAV file.
    """
    audio = midi.fluidsynth(fs)
    sf.write(output_path, audio, fs)


def create_jsonl_file(output_dir: str, job_id: int = 0, total_jobs: int = 1):
    """
    Create a JSONL file from the given data.

    This creates multiple JSONL files, one for every 100k lines.
    They will be saved to the output directory with the same name as
    the output file, but with a number added to the end.
    """

    from src.download import MidiCaps
    from src.tokenizer import MidiTokenizer

    tokenizer = MidiTokenizer()
    ds = MidiCaps(tokenizer)

    os.makedirs(output_dir, exist_ok=True)

    file_idx = job_id
    start = len(ds) // total_jobs * job_id
    limit = len(ds) // total_jobs * (job_id + 1)

    with open(f"{output_dir}/part_{file_idx:05d}.jsonl", "w") as f:
        for i in range(start, limit):
            caption, midi_path = ds[i]

            tokenized_midi = encode_midi_to_tokens(midi_path)
            tokenized_midi = " ".join(tokenized_midi)

            json_obj = {
                "messages": [
                    {
                        "role": "system",
                        "content": "You are a helpful assistant who converts text prompts into MIDI-like tokens.",
                    },
                    {
                        "role": "user",
                        "content": caption,
                    },
                    {
                        "role": "assistant",
                        "content": tokenized_midi,
                    },
                ],
            }

            f.write(json.dumps(json_obj) + "\n")


def finetune(dataset_path: str):
    import torch
    from trl import SFTTrainer
    from transformers import TrainingArguments
    from dotenv import load_dotenv, find_dotenv
    from unsloth.chat_templates import get_chat_template
    from unsloth import FastLanguageModel, is_bfloat16_supported

    load_dotenv(find_dotenv())

    os.environ["WANDB_PROJECT"] = "text2midi-llm"  # name your W&B project
    os.environ["WANDB_LOG_MODEL"] = "checkpoint"  # log all model checkpoints

    max_seq_length = 8192
    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name="unsloth/mistral-7b-v0.3",  # Choose ANY! eg teknium/OpenHermes-2.5-Mistral-7B
        max_seq_length=max_seq_length,
        dtype=None,  # auto
        load_in_4bit=False,
        # token = "hf_...", # use one if using gated models like meta-llama/Llama-2-7b-hf
    )

    model = FastLanguageModel.get_peft_model(
        model,
        r=32,  # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128
        target_modules=[
            "q_proj",
            "k_proj",
            "v_proj",
            "o_proj",
            "gate_proj",
            "up_proj",
            "down_proj",
        ],
        lora_alpha=16,
        lora_dropout=0,  # Supports any, but = 0 is optimized
        bias="none",  # Supports any, but = "none" is optimized
        # [NEW] "unsloth" uses 30% less VRAM, fits 2x larger batch sizes!
        use_gradient_checkpointing="unsloth",  # True or "unsloth" for very long context
        random_state=3407,
        use_rslora=False,  # We support rank stabilized LoRA
        loftq_config=None,  # And LoftQ
    )

    tokenizer = get_chat_template(
        tokenizer,
        chat_template="chatml",  # change this to the right chat_template name
    )

    def formatting_prompts_func(examples):
        convos = examples["messages"]
        texts = [
            tokenizer.apply_chat_template(
                convo, tokenize=False, add_generation_prompt=False
            )
            for convo in convos
        ]
        return {
            "text": texts,
        }

    files = glob.glob(f"{dataset_path}/*.jsonl")
    dataset = load_dataset(
        "json", data_files=files, split="train", cache_dir="./.cache"
    )
    dataset = dataset.map(
        formatting_prompts_func,
        batched=True,
    )

    trainer = SFTTrainer(
        model=model,
        tokenizer=tokenizer,
        train_dataset=dataset,
        dataset_num_proc=2,
        packing=False,  # Can make training 5x faster for short sequences.
        args=TrainingArguments(
            per_device_train_batch_size=2,
            gradient_accumulation_steps=4,
            warmup_steps=5,
            num_train_epochs=1,
            # max_steps=60,  # Set num_train_epochs = 1 for full training runs
            learning_rate=2e-4,
            fp16=not is_bfloat16_supported(),
            bf16=is_bfloat16_supported(),
            logging_steps=1,
            optim="adamw_8bit",
            weight_decay=0.01,
            lr_scheduler_type="linear",
            seed=3407,
            output_dir="outputs",
            save_strategy="steps",
            save_steps=50,
            report_to="wandb",  # Use this for WandB etc
        ),
    )

    trainer_stats = trainer.train()
    # trainer_stats = trainer.train(resume_from_checkpoint = True)

    model.save_pretrained("lora_model")  # Local saving
    tokenizer.save_pretrained("lora_model")


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser()
    parser.add_argument("--jsonl", action="store_true")

    # The directory to save the JSONL files.
    parser.add_argument("--jsonl-dir", type=str, default="jsonl_data", required=False)

    # This is used as an offset (you can skip lines so we can batch)
    parser.add_argument("--jsonl-job-id", type=int, default=0)
    parser.add_argument("--jsonl-total-jobs", type=int, default=1)

    parser.add_argument("--finetune", action="store_true")
    args = parser.parse_args()

    if args.jsonl:
        create_jsonl_file(
            args.jsonl_dir,
            job_id=args.jsonl_job_id,
            total_jobs=args.jsonl_total_jobs,
        )

    if args.finetune:
        finetune(args.jsonl_dir)
